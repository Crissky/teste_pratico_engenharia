{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYGl0ImZSjyN"
   },
   "source": [
    "# Comandos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6I8TqENZ4QzX",
    "outputId": "1799e469-2109-49cb-fbc8-463ce4bf040e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo[srv] in c:\\users\\pg200\\anaconda3\\lib\\site-packages (3.11.4)\n",
      "Requirement already satisfied: dnspython<2.0.0,>=1.16.0; extra == \"srv\" in c:\\users\\pg200\\anaconda3\\lib\\site-packages (from pymongo[srv]) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo[srv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cxCU9VqLRAV"
   },
   "source": [
    "# Funções\n",
    "- **connectDB()**\n",
    "    - Argumentos:\n",
    "        - **url_connection**: String de conexão para um banco \"mongodb\".\n",
    "    - Retorno:\n",
    "        - **pymongo.mongo_client.MongoClient**.\n",
    " \n",
    "- **getDatabase()**\n",
    "    - Argumentos:\n",
    "        - **client**: MongoClient.\n",
    "        - **database_name**: String com o nome da banco de dados.\n",
    "    - Retorno:\n",
    "        - **Database** (Já configurado para a banco de dados \"database_name\").\n",
    "    \n",
    "- **getCollection()**\n",
    "    - Argumentos:\n",
    "        - **database**: MongoClient já configurado para uma banco de dados.\n",
    "        - **collection_name**: String com o nome da coleção.\n",
    "    - Retorno:\n",
    "        - **pymongo.collection.Collection** (Já configurado para a coleção \"collection_name\" no banco de dados \"database\").\n",
    "\n",
    "- **getJson()**\n",
    "    - Argumentos:\n",
    "        - **url**: String com URL para um arquivo json.\n",
    "    - Retorno:\n",
    "        - **List** | **Dict**: Objeto python (Lista ou Dicionário) que melhor represente o json.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w7k-U5pa_Fuu"
   },
   "outputs": [],
   "source": [
    "def connectDB(url_connection):\n",
    "    from pymongo import MongoClient\n",
    "    client = MongoClient(url_connection)\n",
    "\n",
    "    return client\n",
    "\n",
    "def getDatabase(client, database_name='myFirstDatabase'):\n",
    "    return client[database_name]\n",
    "\n",
    "def getCollection(database, collection_name='dataset-0'):\n",
    "    return database[collection_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "UaLpJyue71MM"
   },
   "outputs": [],
   "source": [
    "def getJson(url):\n",
    "    import requests\n",
    "\n",
    "    response = requests.get(url)\n",
    "    my_json = response.json()\n",
    "    \n",
    "    return my_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9MFGeSZLUrr"
   },
   "source": [
    "# Parâmetros\n",
    "- **url_connection**: String de conexão para um banco \"mongodb\".\n",
    "- **url_jsons**: Lista com das URLs dos jsons disponibilizados no github do teste.\n",
    "- **url_json**: URL do json que será submetido ao teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FNjuckhu3KtK"
   },
   "outputs": [],
   "source": [
    "url_connection = \"mongodb://localhost:27017/mydb\"\n",
    "url_jsons = [\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-0.json\",\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-1.json\",\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-2.json\",\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-3.json\",\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-4.json\",\n",
    "             \"https://s3.amazonaws.com/intelivix-datasets/testes_praticos/dataset-5.json\"\n",
    "]\n",
    "url_json = url_jsons[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzo1UYqpLal-"
   },
   "source": [
    "# Instâncias\n",
    "\n",
    "Instanciando o **MongoClient -> Database -> Collection** nas três primeiras células abaixo.\n",
    "\n",
    "O **json_list** da quarta célula recebe o objeto python que representa o json a partir da url, **url_json**. Uma lista de dicionários para este caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mFr_BZsbAglR",
    "outputId": "9fda8b35-d97c-4fb7-bbbf-00e5dcb9b746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = connectDB(url_connection)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1cYkmAeIEw6",
    "outputId": "79b29ff2-e813-4ade-cff1-9ba0e121f20b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'myFirstDatabase')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = getDatabase(client=client)\n",
    "database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kV9QK7VJn3M",
    "outputId": "9035060b-5ec6-469f-da00-caf368b57632"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'myFirstDatabase'), 'dataset-0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = getCollection(database=database)\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4iTttVv7tBm",
    "outputId": "97c5d708-d554-43c2-f753-7df995cce0e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_list = getJson(url_json)\n",
    "len(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8VJ0UHUesx4"
   },
   "source": [
    "# 1\\. Carregar uma das bases de dados listadas acima em um banco de dados MongoDB.\n",
    "\n",
    "- A primeira célula **apaga** todos os dados na coleção, **collection**, e depois faz um contagem do total de documentos na coleção para confirmar a exclusão.\n",
    "\n",
    "- A segunda célula modifica o \"id\" dos processos para \"_id\", conforme o padrão do MongoDB.\n",
    "- A terceira célula insere os dados do **json_list** na coleção, **collection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.drop()\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YPDEdvYkQo08"
   },
   "outputs": [],
   "source": [
    "for my_dict in json_list:\n",
    "    my_dict['_id'] = my_dict.pop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "p3BCkLniJ0HR"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    result = collection.insert_many(json_list)\n",
    "    type(result)\n",
    "except NameError:\n",
    "    print(\"Error: \", NameError)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGH2kxMge6Lg"
   },
   "source": [
    "# 2\\. Responder as seguintes consultas:\n",
    "\n",
    "1. Contagem total dos processos.\n",
    "1. Contagem total dos andamentos.\n",
    "1. Contagem de processos por estado.\n",
    "1. Contagem de juízes que começam com 'S'.\n",
    "1. Contagem de etiquetas mais comuns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKBSX-Ydl_v7"
   },
   "source": [
    "## 1 Contagem total dos processos.\n",
    "\n",
    "Contando todos com documentos (processos) na coleção com **count_documents()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "etML9YTnTN-S",
    "outputId": "3e8dd44b-cd5a-4689-dd7b-f60804f9a3f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de processos: 1000\n"
     ]
    }
   ],
   "source": [
    "total_documents = collection.count_documents({})\n",
    "print(\"Total de processos:\", total_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZaanyonmY2e"
   },
   "source": [
    "## 2 Contagem total dos andamentos.\n",
    "\n",
    "- A primeira célula conta os documentos fazendo uma busca por todos com **find({},{'andamentos':1})**, que retorna uma coleção com os andamentos de cada processo. O total de andamentos de cada processo é armazenado na variável **count_andamentos** por meio de um laço.\n",
    "\n",
    "- A segunda célula faz uma agregação dos andamentos com **aggregate()**, que, neste caso, vai retornar uma coleção com o total de andamentos de cada processo. Esses valores são somados na variável **count_andamentos** em um laço.\n",
    "\n",
    "As duas células exibem o mesmo resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fvUNKEI4mbBF",
    "outputId": "7b7ca822-fbf1-4813-b85e-0437635fc27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dos andamentos: 42285\n"
     ]
    }
   ],
   "source": [
    "count_andamentos = 0\n",
    "for document in collection.find({},{'andamentos':1}):\n",
    "    count_andamentos += len(document['andamentos'])\n",
    "\n",
    "print(\"total dos andamentos:\", count_andamentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jArF0xLAvn52",
    "outputId": "d7a2ceff-aa63-4fc3-d2e7-200eded30039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total dos andamentos: 42285\n"
     ]
    }
   ],
   "source": [
    "count_andamentos = 0\n",
    "pipeline = [{\n",
    "    \"$project\":{\n",
    "        \"length_array\":{\n",
    "            \"$size\":\"$andamentos\"\n",
    "            }\n",
    "        }\n",
    "    }]\n",
    "\n",
    "agg_cursor = collection.aggregate(pipeline)\n",
    "\n",
    "for line in agg_cursor:\n",
    "    count_andamentos += line['length_array']\n",
    "\n",
    "print(\"total dos andamentos:\", count_andamentos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZYiLxEFwTIU"
   },
   "source": [
    "## 3 Contagem de processos por estado.\n",
    "\n",
    "Para essa contagem foi usada uma agregação, **aggregate()**, pela soma de ocorrências do atributo \"estado\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVwrHyQnwUb1",
    "outputId": "0cf2d030-52b2-441e-c09c-fd13324596b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estado: AC, Total: (41)\n",
      "Estado: AL, Total: (28)\n",
      "Estado: AM, Total: (26)\n",
      "Estado: AP, Total: (48)\n",
      "Estado: BA, Total: (37)\n",
      "Estado: CE, Total: (48)\n",
      "Estado: DF, Total: (36)\n",
      "Estado: ES, Total: (44)\n",
      "Estado: GO, Total: (40)\n",
      "Estado: MA, Total: (37)\n",
      "Estado: MG, Total: (34)\n",
      "Estado: MS, Total: (40)\n",
      "Estado: MT, Total: (24)\n",
      "Estado: PA, Total: (29)\n",
      "Estado: PB, Total: (29)\n",
      "Estado: PE, Total: (40)\n",
      "Estado: PI, Total: (30)\n",
      "Estado: PR, Total: (40)\n",
      "Estado: RJ, Total: (31)\n",
      "Estado: RN, Total: (49)\n",
      "Estado: RO, Total: (42)\n",
      "Estado: RR, Total: (44)\n",
      "Estado: RS, Total: (37)\n",
      "Estado: SC, Total: (36)\n",
      "Estado: SE, Total: (33)\n",
      "Estado: SP, Total: (29)\n",
      "Estado: TO, Total: (48)\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    { \"$group\": {'_id': '$estado', \"total\": {'$sum':1} } },\n",
    "    { \"$sort\": {'_id': 1} }\n",
    "]\n",
    "\n",
    "cursor = collection.aggregate(pipeline)\n",
    "\n",
    "for line in cursor:\n",
    "    print('Estado: {}, Total: ({})'.format(line['_id'], line['total']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMFUmbg7mCr6"
   },
   "source": [
    "## 4 Contagem de juízes que começam com 'S'.\n",
    "\n",
    "A contagem dos juízes foi feita usando um **regex** como parâmetro na função **count_documents** para filtrar somente os nomes dos juízes que iniciam com a letra 'S' (somente maiúsculo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HOASc9CufiLM",
    "outputId": "fbc222f1-a58d-4188-f724-17c1a913bf15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Juízes que começam com a letra 'S': 67\n"
     ]
    }
   ],
   "source": [
    "total_judge_start_s = collection.count_documents( {'juiz':{\"$regex\": \"^S\"}} )\n",
    "print(\"Juízes que começam com a letra 'S':\", total_judge_start_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZJOiaRb2ket"
   },
   "source": [
    "## 5 Contagem de etiquetas mais comuns.\n",
    "\n",
    "Para a contagem do total de ocorrências de cada etiqueta foi usada uma agregação, **aggregate()**, somando a ocorrência de cada etiqueta. Como as etiquetas são listas, assim como os andamentos, e elas são atributos de andamento foi necessário o argumento **\"$unwind\"**, em cada, para realizar a contagem corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiAfwBSL2m3t",
    "outputId": "4e06e6eb-c4db-40d8-d1ce-b57d6e5d0250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etiqueta: Brown, Total: (9924)\n",
      "Etiqueta: Yellow, Total: (9922)\n",
      "Etiqueta: Pink, Total: (9919)\n",
      "Etiqueta: Green, Total: (9908)\n",
      "Etiqueta: Black, Total: (9845)\n",
      "Etiqueta: Red, Total: (9831)\n",
      "Etiqueta: Magenta, Total: (9776)\n",
      "Etiqueta: Blue, Total: (9772)\n",
      "Etiqueta: Beige, Total: (9767)\n",
      "Etiqueta: Orange, Total: (9759)\n",
      "Etiqueta: White, Total: (9729)\n",
      "Etiqueta: Purple, Total: (9711)\n",
      "Etiqueta: Cyan, Total: (9613)\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    { \"$unwind\": \"$andamentos\" },\n",
    "    { \"$unwind\": \"$andamentos.etiquetas\" },\n",
    "    { \"$group\": {\"_id\": \"$andamentos.etiquetas\", \"total\": {\"$sum\": 1} } },\n",
    "    { \"$sort\": {'total': -1} }\n",
    "]\n",
    "\n",
    "cursor = collection.aggregate(pipeline)\n",
    "\n",
    "for line in cursor:\n",
    "    print('Etiqueta: {}, Total: ({})'.format(line['_id'], line['total']))\n",
    "    # print(\"Etiqueta:\", line['_id'], \"- Total:\", line[\"total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2 in c:\\users\\pg200\\anaconda3\\lib\\site-packages (2.8.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Column, Integer, String, Date, ForeignKey, Boolean\n",
    "from sqlalchemy import create_engine, Table\n",
    "from sqlalchemy import and_, or_\n",
    "from sqlalchemy import asc, desc\n",
    "from sqlalchemy import extract\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.sql.expression import func\n",
    "\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função **recreate_database** apaga e recria todas as tabelas incluidas no sqlalchemy.orm.decl_api.Base. Relevante para quando o código for re-executado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recreate_database():\n",
    "    Base.metadata.drop_all(engine) # Para destruir esta tabela (e todas as tabelas) no banco de dados\n",
    "    Base.metadata.create_all(engine) # Cria a tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **DATABASE_URI**: URL de conexão com o banco Postgre.\n",
    "- **engine**: Objeto do tipo **Engine** usando para criar o objeto do tipo **sessionmaker**\n",
    "- **Session**: Objeto do tipo **sessionmaker** usando para criar sessões com o Postgre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_URI = 'postgresql+psycopg2://postgres:senha123@localhost:5432/teste_pratico_engenharia'\n",
    "engine = create_engine(DATABASE_URI)\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**session_scope()** gerencia a criação, o envio e o fechamento usando a biblioteca **contextlib**. Caso ocorra algum erro, ele também faz o **rollback** (defaz) das alterações executadas dentro de seu escopo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def session_scope():\n",
    "    from time import time\n",
    "    start = time()\n",
    "    session = Session()\n",
    "    try:\n",
    "        yield session\n",
    "        session.commit()\n",
    "    except Exception:\n",
    "        session.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "        end = time()\n",
    "        print('\\nTempo total da operação: {0:.2f} segundos'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- A primeira célula instancia um objeto do tipo **Base** que vai gerenciar as tabelas com base nas classes que o herdam.\n",
    "\n",
    "- A segunda célula define as classes **Processo** e **Andamento** que representam as tabelas \"processos\" e \"andamentos\", respectivamente.\n",
    "\n",
    "- A terceira célula chama a função **recreate_database** para recriar as tabelas do banco de dados.\n",
    "\n",
    "*As novas colunas \"total_andamento_tratado\" e \"texto_contem_cinema\" já foram incluídas nas tabelas para não precisar usar biblioteca Alembic, com o intuito de facilitar o uso do notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = declarative_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processo(Base):\n",
    "    __tablename__ = 'processos'\n",
    "    _id = Column(String, primary_key=True)\n",
    "    npu = Column(String)\n",
    "    estado = Column(String)\n",
    "    spider = Column(String)\n",
    "    juiz = Column(String)\n",
    "    data_distribuicao = Column(Date)\n",
    "    data_captura = Column(Date)\n",
    "    andamentos = relationship('Andamento', back_populates='processo')\n",
    "    total_andamento_tratado = Column(Integer, default=0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Processo(id={}, npu={}, estado={}, spider={}, juiz={}, data_distribuicao={}, data_captura={}, total_andamento_tratado={})>\"\\\n",
    "                .format(self._id, self.npu, self.estado, self.spider, self.juiz, self.data_distribuicao, self.data_captura, self.total_andamento_tratado)\n",
    "\n",
    "class Andamento(Base):\n",
    "    __tablename__ = 'andamentos'\n",
    "    _id = Column(Integer, primary_key=True)\n",
    "    texto = Column(String)\n",
    "    data = Column(Date)\n",
    "    etiquetas = Column(String)\n",
    "    processo_id = Column(String, ForeignKey('processos._id'))\n",
    "    processo = relationship(\"Processo\", back_populates=\"andamentos\")\n",
    "    texto_contem_cinema = Column(Boolean, default=False)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Andamento(id={}, data={}, etiquetas={}, processo_id={}, texto_contem_cinema={}, texto={})>\"\\\n",
    "                .format(self._id, self.data, self.etiquetas, self.processo_id, self.texto_contem_cinema, self.texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recreate_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\\. Lendo os dados a partir do MongoDB transformando-os e carregar o resultado em uma tabela do PostgreSQL.\n",
    "\n",
    "- Gerar 2 modelos (Processo e Andamento) usando SQLAlchemy. Inferir os campos através do esquema apresentado acima. O candidato tem liberdade para criar novos campos para lhe ajudar nas tarefas.\n",
    "    \n",
    "```\n",
    "{\n",
    "    \"id\": \"263c9996-5f74-6412-e01f-cbecdca71c5e\",\n",
    "    \"npu\": \"1517345-36.2016.8.01.0560\",\n",
    "    \"estado\": \"PB\",\n",
    "    \"spider\": \"projudi-rn\",\n",
    "    \"juiz\": \"Saulo Braga Santana Falcão\",\n",
    "    \"data_distribuicao\": \"1990-09-06T14:08:01Z\",\n",
    "    \"data_captura\": \"2017-12-27T17:04:29Z\",\n",
    "    \"andamentos\": [\n",
    "        {\n",
    "            \"texto\": \"denmark quest strip upgrade rocky ... opportunity\",\n",
    "            \"data\": \"1993-12-02T15:40:49Z\",\n",
    "            \"etiquetas\": [\n",
    "                \"Yellow\",\n",
    "                \"Pink\",\n",
    "                \"Magenta\"\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o **session_scope()** para abrir uma \"sessão\", é feito um laço baseado na consulta de todos os documentos no **MongoDB** para instanciar um objeto \"processo\" e todos os objetos \"andamentos\" que pertencem ao \"processo\" a cada iteração do laço. Após isso, o \"processo\" é adicionado a \"sessão\" que insere os dados nas tabelas do **Postgre** quando o laço for concluído.\n",
    "\n",
    "*Todas as interações com o banco de dados Postgre serão feitas usando o **session_scope()**, então esse passo será omitido*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total da operação: 21.08 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    for line in collection.find({}):\n",
    "        andamentos = line.pop('andamentos')\n",
    "        processo = Processo(**line)\n",
    "\n",
    "        for andamento in andamentos:\n",
    "            andamento['processo_id'] = line['_id']\n",
    "            andamento['etiquetas'] = ', '.join(andamento['etiquetas'])\n",
    "            andamento = Andamento(**andamento)\n",
    "\n",
    "            processo.andamentos.append(andamento)\n",
    "\n",
    "        session.add(processo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizar as transformações abaixo:\n",
    "1. Deixar somente o primeiro e último nome dos Juízes.\n",
    "1. Remover todos os andamentos cuja data for anterior a data de distribuição.\n",
    "1. Modificar os npus que não possuam um ano entre 1980 e 2018 para o ano 2000. (Ex: 1517345-36.6416.8.01.0560 vira 1517345-36.2000.8.01.0560)\n",
    "1. Remover todas as palavras que comecem com a letra 'r' dos textos dos andamentos.\n",
    "1. Adicionar um campo inteiro no modelo de Processo com a quantidade de andamentos (somente os válidos que já foram transformados).\n",
    "1. Adicionar um campo booleano no modelo de Andamento que verifique se a palavra cinema esta no texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. Deixar somente o primeiro e último nome dos Juízes.\n",
    "\n",
    "É feito um laço em uma consulta na tabela processos que faz alguns tratamentos no campo \"juiz\" dentro do objeto \"processo\".\n",
    "1. Remove todos os espaços em excesso (espaços duplos, triplos...) e espaços no início e final do campo.\n",
    "1. Transforma a **String** em uma lista, onde cada elemento da lista é um nome do juiz.\n",
    "1. Cria uma nova **String** usando o primeiro e último elemento da lista (primeiro e último nome).\n",
    "1. Insere a nova **String** no objeto processo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total da operação: 0.13 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    for processo in session.query(Processo).all():\n",
    "        juiz = re.sub(' +', ' ', processo.juiz).strip()\n",
    "        juiz = juiz.split()\n",
    "        juiz = f'{juiz[0]} {juiz[-1]}'\n",
    "        processo.juiz = juiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. Remover todos os andamentos cuja data for anterior a data de distribuição.\n",
    "\n",
    "Uma \"query\" é feita usando o atributo **.\\_\\_table\\_\\_.** da classe **Andamento** para chamar seus métodos **delete** e **where**.\n",
    "Como argumento do **where** é passado a função **and_** do SQLAlchemy com as seguinte comparações:\n",
    "- **Andamento.processo_id == Processo._id**\n",
    "- **Andamento.data < Processo.data_distribuicao**\n",
    "\n",
    "Após isso, a \"query\" é executada por meio do método **execute** do objeto \"session\" que retorna um objeto do tipo **cursor** que contém o resultado da consulta. Por fim, o total de linhas afetadas (excluídas) é impressa.\n",
    "\n",
    "*O código comentado da segunda célula é uma outra maneira de fazer a remoção com o método **delete** do objeto \"session\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Andamentos excluídos: 21453\n",
      "\n",
      "Tempo total da operação: 0.19 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    delete_query = Andamento.__table__.delete().where(\n",
    "        and_(\n",
    "                Andamento.processo_id == Processo._id,\n",
    "                Andamento.data < Processo.data_distribuicao\n",
    "        )\n",
    "    )\n",
    "    cursor_result = session.execute(delete_query)\n",
    "    \n",
    "    print('Total de Andamentos excluídos:', cursor_result.rowcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with session_scope() as session:\n",
    "#     andamentos = session.query(Andamento).filter(\n",
    "#         and_(\n",
    "#             Andamento.processo_id == Processo._id,\n",
    "#             Andamento.data < Processo.data_distribuicao\n",
    "#         )\n",
    "#     ).all()\n",
    "#     count = len(andamentos)\n",
    "#     for andamento in andamentos:\n",
    "#         session.delete(andamento)\n",
    "#     print('Total de Andamentos excluídos:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\\. Modificar os npus que não possuam um ano entre 1980 e 2018 para o ano 2000. (Ex: 1517345-36.6416.8.01.0560 vira 1517345-36.2000.8.01.0560)\n",
    "\n",
    "Em um laço feito na tabela \"processos\" é feita as seguintes alterações no campo \"NPU\":\n",
    "- A **String** NPU é convertida em uma lista, onde os \"cortes\" são feitos nos pontos \".\".\n",
    "- O segundo valor da lista, que equivale ao ano, é convertido em inteiro e armazenado na variável **npu_year**.\n",
    "- Caso o **npu_year** seja menor que 1980 ou maior que 2018, segundo valor da lista será substituído pela **String** \"2000\".\n",
    "- Por fim, a lista é transformada novamente em **String**, onde cada elemento dela é separado por ponto \".\". Após isso, a **String** é atribuída ao objeto \"processo\" que irá atualizar a tabela \"processos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de NPUs alteradas: 995\n",
      "\n",
      "Tempo total da operação: 0.36 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    count = 0\n",
    "    for processo in session.query(Processo).all():\n",
    "        npu = processo.npu\n",
    "        npu = npu.split('.')\n",
    "        npu_year = int(npu[1])\n",
    "\n",
    "        if(npu_year < 1980 or npu_year > 2018):\n",
    "            count += 1\n",
    "            npu[1] = '2000'\n",
    "            \n",
    "        npu = '.'.join(npu)\n",
    "        processo.npu = npu\n",
    "        \n",
    "    print('Total de NPUs alteradas:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\\. Remover todas as palavras que comecem com a letra 'r' dos textos dos andamentos.\n",
    "\n",
    "### 5\\. Adicionar um campo inteiro no modelo de Processo com a quantidade de andamentos (somente os válidos que já foram transformados).\n",
    "\n",
    "Antes do laço na tabela \"andamentos\" uma expressão regular é definida para retirar todas as palavras iniciadas com \"r\" (minúsculo ou maiúsculo). Já no laço, caso o texto de um \"andamento\" contenha uma palavra iniciada com \"r\", ele será substituído pelo mesmo texto (sem as palavras iniciadas em \"r\") e o campo \"total_andamento_tratado\" do processo que possui o andamento em questão é incrementado em 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total da operação: 54.44 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    regex = re.compile(r'(\\s)[rR]\\w+')\n",
    "    for andamento in session.query(Andamento).all():\n",
    "        if regex.search(andamento.texto):\n",
    "            andamento.texto = regex.sub('', andamento.texto)\n",
    "            processo = session.query(Processo).filter(Processo._id == andamento.processo_id).first()\n",
    "            processo.total_andamento_tratado += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\\. Adicionar um campo booleano no modelo de Andamento que verifique se a palavra cinema esta no texto.\n",
    "\n",
    "Em um laço pela tabela \"andamentos\" uma expressão regular verifica se há ao menos uma ocorrência da palavra cinema no campo \"texto\". Caso haja, o campo \"texto_contem_cinema\" do andamento em questão é atualizado para \"True\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total da operação: 2.34 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    for andamento in session.query(Andamento).all():\n",
    "        if re.search(r'\\bcinema\\b', andamento.texto.lower()):\n",
    "            andamento.texto_contem_cinema = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\. Responder as seguintes consultas pós-processamento:\n",
    "1. Qual o total de processos? Qual o total de andamentos?\n",
    "1. Qual processo possui mais andamentos?\n",
    "1. Quais andamentos possuem mais caracteres? Quais são os seus processos?\n",
    "1. Qual andamento mais antigo com o termo \"cinema\"?\n",
    "1. Qual processo possui o maior número formado pelos seus 6 primeiros números do seu npu?\n",
    "1. Qual mês/ano foram capturados mais processos para cada \"spider\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\\. Qual o total de processos? Qual o total de andamentos?\n",
    "\n",
    "Foi feita duas contagem pelo total de elementos na tabela \"processos\" e \"andamentos\". Em seguida esses valores são \"printados\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de Processos: 1000\n",
      "Total de Andamentos: 20832\n",
      "\n",
      "Tempo total da operação: 0.04 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    total_processo = session.query(Processo).count()\n",
    "    total_andamento = session.query(Andamento).count()\n",
    "\n",
    "    print(f'Total de Processos: {total_processo}')\n",
    "    print(f'Total de Andamentos: {total_andamento}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\\. Qual processo possui mais andamentos?\n",
    "\n",
    "Foi realizada uma contagem na tabela \"andamentos\" agrupado pelo campo \"processo_id\", que contém o \"_id\" do processo que o andamento pertence, e ordenado de forma decrescente. Após isso, o primeiro valor da contagem é impresso.\n",
    "\n",
    "*A segunda célula faz a mesma contagem recebendo uma lista com todos os processos (objetos) e ordenando-os (decrescente) pelo número de andamentos que eles possuem. Após isso, o número de andamentos do primeiro processo da lista e seu \"_id\" são impressos*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo._id: 3f6beb82-3d4f-c2b0-2d84-7e249cffea9c, Número de Andamentos: 76\n",
      "\n",
      "Tempo total da operação: 0.02 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    count_andamentos_per_processo = session.query(\n",
    "                                                func.count(Andamento.processo_id).label('total_processo_id'),\n",
    "                                                Andamento.processo_id)\\\n",
    "                                    .group_by(Andamento.processo_id)\\\n",
    "                                    .order_by(desc('total_processo_id')).first()\n",
    "    \n",
    "    print(f'Processo._id: {count_andamentos_per_processo[1]}, Número de Andamentos: {count_andamentos_per_processo[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo._id: 3f6beb82-3d4f-c2b0-2d84-7e249cffea9c, Número de Andamentos: 76\n",
      "\n",
      "Tempo total da operação: 11.38 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    processos = session.query(Processo).all()\n",
    "    processos.sort(key=lambda x:len(x.andamentos), reverse=True)\n",
    "    processo = processos[0]\n",
    "    print(f'Processo._id: {processo._id}, Número de Andamentos: {len(processo.andamentos)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3\\. Quais andamentos possuem mais caracteres? Quais são os seus processos?\n",
    "\n",
    "Uma consulta é feita na tabela \"andamentos\" ordenando-os pelo número de caracteres do campo \"texto\". Após isso, é feito um laço para imprimir as informações dos 5 primeiros resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andamento._id:18856, Tamanho do Texto: 2619, Processo._id:6f4284a6-d39a-378f-52b5-59020f4b2850\n",
      "Andamento._id:3586, Tamanho do Texto: 2582, Processo._id:56acfa9e-bb11-c239-b6e4-ee9dd7d54c57\n",
      "Andamento._id:19859, Tamanho do Texto: 2580, Processo._id:22f66a82-5f7b-f0ce-ea4c-72ccff99c3c7\n",
      "Andamento._id:37802, Tamanho do Texto: 2578, Processo._id:e79dff29-ccd2-30ab-7e39-0a8640ad6f55\n",
      "Andamento._id:30556, Tamanho do Texto: 2572, Processo._id:1dcc0995-e804-b95d-c15b-b7539c02ffd3\n",
      "\n",
      "Tempo total da operação: 1.35 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    andamentos = session.query(Andamento).order_by(desc(func.char_length(Andamento.texto))).all()\n",
    "    for index, andamento in enumerate(andamentos[:5]):\n",
    "        print(f'Andamento._id:{andamento._id}, Tamanho do Texto: {len(andamento.texto)}, Processo._id:{andamento.processo_id}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\\. Qual andamento mais antigo com o termo \"cinema\"?\n",
    "\n",
    "Uma consulta foi realizada na tabela \"andamentos\" para buscar somente o primeiro resultado, \"ordenado\" pela data do andamento (crescente), mas somente dos andamentos com o campo \"texto_contem_cinema\" igual a \"True\". Após isso, o objeto \"andamento\", equivalente ao primeiro resultado da consulta, é impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Andamento(id=21024, data=1993-09-06, etiquetas=Red, White, Blue, processo_id=ea7926fc-3ae8-7f17-d084-98552472d3cc, texto_contem_cinema=True, texto=cuts overnight library melbourne false sun employment brad passion look bass intelligent institutions determining wish favor determine toll audio sitting winning contents nokia exploration think goes adventure pupils operational tea ukraine increases acting bulgaria december funds proper celebrity updates possibility mark variation ex atom policies dates controls lo accounting ba thu dependent mercedes chronic sit proposals magazine jersey creek played nose lines espn plug proper sleeve adventure abc affiliated to placement changed programming empire earth interest target silent engineering optical tape exp politics signals decided funding ireland glory pages dennis dialogue unknown closely however board phil activity places saving chase categories stewart standard horses judicial now billing wave engaged then oriented specs species astronomy portugal install changes spray due warning fight keyboard applicant juice keith stereo grown sexual luck clearance championship kings sl its town extent compare dispute taxes nov automobile alberta captain pan postposted july minnesota constraints christmas wife nepal navigation numerous important whereas level province mar andy by para prevention actually original mountain beef interface threat template picture profiles voice connection created apart cached cook improving vi bird designated sue settlement bands xl ace throughout cinema lg sitting clark upload privacy permission hollywood inclusive sweden doubt injuries shelf dodge first mlb integration hp consistent carry horror slightly holding this van support slowly scripts investments catalogue week bearing credits agreements mountains afghanistan minister bachelor performed conservation poverty wonder induced spectrum spanking both sort proteins maker afford sin satisfaction loose partners seller chemical hamilton henry permanent dealtime factory singing can)>\n",
      "\n",
      "Tempo total da operação: 0.01 segundos\n"
     ]
    }
   ],
   "source": [
    "with session_scope() as session:\n",
    "    andamento = session.query(Andamento).filter(Andamento.texto_contem_cinema == True).order_by(Andamento.data.asc()).first()\n",
    "    print(andamento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5\\. Qual processo possui o maior número formado pelos seus 6 primeiros números do seu npu?\n",
    "\n",
    "Uma consulta na tabela \"processos\" é feita para receber uma lista com todos os \"processos\" (objetos). Essa lista é ordenada com base nos 6 primeiros dígitos do campo \"npu\", definido pela função **order_by_npu**. Após a ordenação, o \"_id\" e o \"npu\" do primeiro processo da lista ordenada é impresso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processo._id: f43c8aa0-0dae-3009-302b-f1be29ccb6df, NPU: 9982224-13.2000.5.84.0457\n",
      "\n",
      "Tempo total da operação: 0.04 segundos\n"
     ]
    }
   ],
   "source": [
    "def order_by_npu(processo):\n",
    "    npu = processo.npu\n",
    "    npu = int(npu[:6])\n",
    "    return npu\n",
    "\n",
    "with session_scope() as session:\n",
    "    processos = session.query(Processo).all()\n",
    "    processos.sort(key=order_by_npu, reverse=True)\n",
    "    processo = processos[0]\n",
    "    print(f'Processo._id: {processo._id}, NPU: {processo.npu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6\\. Qual mês/ano foram capturados mais processos para cada \"spider\"?\n",
    "\n",
    "Neste caso, foi feita uma contagem do campo \"spider\" na tabela \"processos\" agrupando pelo campo \"data_captura\" dividindo-o em ano e mês. Após obter o resultado, ele foi ordenado (crescente) pelo valor da contagem e pelo campo \"spider\". Com o resultado ordenado, foi usado o método **filterUniqueSpiders** para deixar somente o maior valor (primeira ocorrência na lista ordenada) para cada \"spider\". Por fim, os valores que permaneceram na lista são impressos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: 8/2016, Spider: esaj-es (4)\n",
      "Data: 10/2017, Spider: projudi-sp (3)\n",
      "Data: 12/2016, Spider: projudi-se (3)\n",
      "Data: 8/2017, Spider: projudi-pa (3)\n",
      "Data: 1/2018, Spider: projudi-al (3)\n",
      "Data: 10/2017, Spider: projudi-ac (3)\n",
      "Data: 4/2018, Spider: pje-rr (3)\n",
      "Data: 5/2016, Spider: pje-pr (3)\n",
      "Data: 10/2016, Spider: pje-ma (3)\n",
      "Data: 9/2018, Spider: pje-df (3)\n",
      "Data: 3/2016, Spider: pje-am (3)\n",
      "Data: 6/2016, Spider: esaj-sp (3)\n",
      "Data: 7/2016, Spider: esaj-sc (3)\n",
      "Data: 11/2016, Spider: esaj-ms (3)\n",
      "Data: 1/2018, Spider: esaj-ba (3)\n",
      "Data: 1/2017, Spider: projudi-to (2)\n",
      "Data: 6/2018, Spider: projudi-sc (2)\n",
      "Data: 11/2017, Spider: projudi-rs (2)\n",
      "Data: 12/2018, Spider: projudi-rr (2)\n",
      "Data: 9/2016, Spider: projudi-ro (2)\n",
      "Data: 12/2017, Spider: projudi-rn (2)\n",
      "Data: 4/2018, Spider: projudi-rj (2)\n",
      "Data: 2/2016, Spider: projudi-pr (2)\n",
      "Data: 7/2017, Spider: projudi-pi (2)\n",
      "Data: 2/2018, Spider: projudi-pe (2)\n",
      "Data: 4/2016, Spider: projudi-mt (2)\n",
      "Data: 4/2018, Spider: projudi-ms (2)\n",
      "Data: 8/2017, Spider: projudi-mg (2)\n",
      "Data: 8/2017, Spider: projudi-ma (2)\n",
      "Data: 7/2016, Spider: projudi-go (2)\n",
      "Data: 7/2018, Spider: projudi-es (2)\n",
      "Data: 7/2018, Spider: projudi-ce (2)\n",
      "Data: 10/2017, Spider: projudi-ba (2)\n",
      "Data: 8/2017, Spider: projudi-ap (2)\n",
      "Data: 5/2016, Spider: projudi-am (2)\n",
      "Data: 12/2017, Spider: pje-to (2)\n",
      "Data: 2/2016, Spider: pje-sp (2)\n",
      "Data: 10/2018, Spider: pje-se (2)\n",
      "Data: 8/2018, Spider: pje-sc (2)\n",
      "Data: 6/2017, Spider: pje-rs (2)\n",
      "Data: 3/2016, Spider: pje-pi (2)\n",
      "Data: 10/2017, Spider: pje-pe (2)\n",
      "Data: 5/2018, Spider: pje-pb (2)\n",
      "Data: 7/2017, Spider: pje-pa (2)\n",
      "Data: 2/2017, Spider: pje-mt (2)\n",
      "Data: 4/2016, Spider: pje-ms (2)\n",
      "Data: 7/2018, Spider: pje-mg (2)\n",
      "Data: 9/2018, Spider: pje-es (2)\n",
      "Data: 3/2017, Spider: pje-ce (2)\n",
      "Data: 8/2016, Spider: pje-ap (2)\n",
      "Data: 4/2018, Spider: pje-al (2)\n",
      "Data: 5/2017, Spider: pje-ac (2)\n",
      "Data: 10/2016, Spider: esaj-to (2)\n",
      "Data: 12/2018, Spider: esaj-se (2)\n",
      "Data: 10/2017, Spider: esaj-rs (2)\n",
      "Data: 2/2016, Spider: esaj-ro (2)\n",
      "Data: 8/2017, Spider: esaj-rn (2)\n",
      "Data: 9/2016, Spider: esaj-rj (2)\n",
      "Data: 3/2016, Spider: esaj-pr (2)\n",
      "Data: 8/2018, Spider: esaj-pi (2)\n",
      "Data: 1/2016, Spider: esaj-pe (2)\n",
      "Data: 10/2016, Spider: esaj-pb (2)\n",
      "Data: 5/2018, Spider: esaj-mt (2)\n",
      "Data: 8/2018, Spider: esaj-mg (2)\n",
      "Data: 8/2018, Spider: esaj-ce (2)\n",
      "Data: 8/2018, Spider: esaj-ap (2)\n",
      "Data: 1/2018, Spider: esaj-al (2)\n",
      "Data: 2/2016, Spider: esaj-ac (2)\n",
      "Data: 1/2017, Spider: projudi-pb (1)\n",
      "Data: 2/2016, Spider: projudi-df (1)\n",
      "Data: 8/2016, Spider: pje-ro (1)\n",
      "Data: 5/2018, Spider: pje-rn (1)\n",
      "Data: 3/2017, Spider: pje-rj (1)\n",
      "Data: 5/2018, Spider: pje-go (1)\n",
      "Data: 9/2017, Spider: pje-ba (1)\n",
      "Data: 6/2016, Spider: esaj-rr (1)\n",
      "Data: 10/2018, Spider: esaj-pa (1)\n",
      "Data: 11/2018, Spider: esaj-ma (1)\n",
      "Data: 6/2018, Spider: esaj-go (1)\n",
      "Data: 5/2018, Spider: esaj-df (1)\n",
      "Data: 8/2018, Spider: esaj-am (1)\n",
      "\n",
      "Tempo total da operação: 0.03 segundos\n"
     ]
    }
   ],
   "source": [
    "def filterUniqueSpiders(my_list):\n",
    "    found_list = list()\n",
    "    new_list = list()\n",
    "    for line in my_list:\n",
    "        if(not line[1] in found_list):\n",
    "            found_list.append(line[1])\n",
    "            new_list.append(line)\n",
    "    \n",
    "    return new_list\n",
    "\n",
    "with session_scope() as session:\n",
    "    spiders = session.query(func.count(Processo.spider),\\\n",
    "                            Processo.spider,\\\n",
    "                            extract('year', Processo.data_captura),\\\n",
    "                            extract('month', Processo.data_captura))\\\n",
    "    .group_by(Processo.spider,\\\n",
    "              extract('year', Processo.data_captura),\\\n",
    "              extract('month', Processo.data_captura))\\\n",
    "    .all()\n",
    "                    \n",
    "    spiders.sort(key=lambda x:(x[0],x[1]), reverse=True)\n",
    "    spiders = filterUniqueSpiders(spiders)\n",
    "    for spider in spiders:\n",
    "        print(f'Data: {int(spider[3])}/{int(spider[2])}, Spider: {spider[1]} ({spider[0]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\\. Ao final exportar as tabelas do banco de dados PostgreSQL para um arquivo chamado report.csv (delimitador de texto ' \" ', separador ' | ').\n",
    "\n",
    "Para transformar as tabelas em CSV foi usado a biblioteca CSV do python com o módulo **DictWriter** que recebe cada linha em forma de dicionário. O argumento **fieldnames** recebe uma lista com as chaves do dicionário que usaremos para escrever no CSV.\n",
    "\n",
    "Para criar essa lista usei a função **getFieldNames** que transforma os nomes dos atributos de um objeto em uma lista com base no atributo **\\_\\_dict\\_\\_** do próprio objeto. O método também tira a chave \"\\_sa\\_instance\\_state\", desnecessária para a operação. O método foi usado em uma instância das classes **Processo** e **Andamento**, gerando duas listas que foram unidas e usadas no argumento **fieldnames**.\n",
    "\n",
    "Em seguida, um laço pela tabela \"andamentos\" transforma os objetos \"andamento\" em dicionários usando a função **sqaObjToDict**, da mesma maneira o objeto \"processo\" que o \"andamento\" pertence também é transformado em dicionário. Esses dois dicionários (andamento e processo) são unidos e escritos no CSV.\n",
    "\n",
    "Cada linha do CSV representa um andamento e o seu respectivo processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total da operação: 22.11 segundos\n"
     ]
    }
   ],
   "source": [
    "def getFieldNames(my_sqa_obj):\n",
    "    my_dict = dict(my_sqa_obj.__dict__)\n",
    "    my_field_names = my_dict.keys()\n",
    "    my_field_names = list(my_field_names)\n",
    "    my_field_names.remove('_sa_instance_state')\n",
    "    \n",
    "    return my_field_names\n",
    "\n",
    "def sqaObjToDict(my_sqa_obj):\n",
    "    my_dict = dict(my_sqa_obj.__dict__)\n",
    "    my_dict.pop('_sa_instance_state')\n",
    "    \n",
    "    return my_dict\n",
    "    \n",
    "with session_scope() as session:\n",
    "    processo = session.query(Processo).first()\n",
    "    andamento = session.query(Andamento).first()\n",
    "    \n",
    "    processo_keys_list = getFieldNames(processo)\n",
    "    processo_keys_list.sort()\n",
    "    andamento_keys_list = getFieldNames(andamento)\n",
    "    andamento_keys_list.sort()\n",
    "    \n",
    "    andamento_keys_list.remove('processo_id')\n",
    "    andamento_keys_list.remove('_id')\n",
    "    \n",
    "    processo_andamento_keys_list = processo_keys_list + andamento_keys_list\n",
    "      \n",
    "    file = open('report.csv', 'w', newline='')\n",
    "    writer = csv.DictWriter(file, fieldnames=processo_andamento_keys_list, delimiter='|', quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for andamento in session.query(Andamento).all():\n",
    "        andamento_dict = sqaObjToDict(andamento)\n",
    "        andamento_dict.pop('processo_id')\n",
    "        andamento_dict.pop('_id')\n",
    "        processo = session.query(Processo).filter(Processo._id == andamento.processo_id).first()\n",
    "        \n",
    "        processo_dict = sqaObjToDict(processo)\n",
    "        processo_dict.update(andamento_dict)\n",
    "        \n",
    "        writer.writerow(processo_dict)\n",
    "        \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sobre a execução do teste\n",
    "\n",
    "Antes de começar a escrever código, tanto da parte de MongoDB quanto de PostgreSQL, eu precisei ler a documentação introdutória e executar um tutorial básico, pois eu não tinha conhecimento de nenhuma das duas bibliotecas, PyMongo e SQLAlchemy. Também houveram dúvidas para executar alguns dos testes, o que exigiu uma pesquisa adicional sobre as ferramentas.\n",
    "\n",
    "Também perdi algum tempo para preparar o ambiente. A princípio usei o Google Colab para fazer a parte do MongoDB, pois era só conectar a um banco gratuito no Mongo Atlas, ou qualquer outro, e começar a escrever o código.\n",
    "\n",
    "Após terminar essa primeira parte, tentei usar o docker (também não tenho familiaridade com ele) no próprio Google Colab, pois a máquina dele e a internet são melhores que a minha — com isso eu conseguiria executar os códigos bem mais rápido com os json maiores. Depois de um tempo, eu descobri que o serviço não oferece suporte ao docker. Então eu usei o docker na minha máquina, mas tive problemas com a imagem do Postgre que não estava executando corretamente. Passei um tempo tentando fazer a imagem do Postgre funcionar, como não consegui, eu decidi baixar o SGBD deles.\n",
    "\n",
    "O tempo que usei para realizar as atividades foi pela parte da noite, de terça a sexta. E hoje eu usei para escrever as notas no notebook e modificar duas células de código dele.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kyS3yL24wZXa"
   ],
   "name": "teste_pratico_engenharia.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
